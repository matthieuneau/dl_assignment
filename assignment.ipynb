{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## I. VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1:\n",
    "\n",
    "We sample an image by following the process of **ancestral sampling**, meaning that we sample by following the dependencies defined by our model. The process is as follows:\n",
    "\n",
    "1. **Sample the latent variable $z_n$:**\n",
    "\n",
    "   $$\n",
    "   z_n \\sim \\mathcal{N}(0, \\mathbf{I}_d)\n",
    "   $$\n",
    "\n",
    "   where $\\mathbf{I}_d$ is the identity matrix.\n",
    "\n",
    "2. **Compute the output of the decoder:**\n",
    "\n",
    "   $$\n",
    "   f_\\theta(z_n)\n",
    "   $$\n",
    "\n",
    "   where $f_\\theta$ is the decoder network parameterized by $\\theta$.\n",
    "\n",
    "3. **Sample the pixels of the image:**\n",
    "\n",
    "   $$\n",
    "   x_m \\sim \\mathcal{B}(f_\\theta(z_n)_m)\n",
    "   $$\n",
    "\n",
    "   where $\\mathcal{B}$ represents a Bernoulli distribution, and $f_\\theta(z_n)_m$ gives the probability of each pixel $m$.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Note**:\n",
    "In step 3, **all pixels can be sampled in parallel**, as the pixels are independent by definition in our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2:\n",
    "\n",
    "This method is inefficient because evaluating $p(x_n \\mid z_n)$ is costly, and the cost increases as the dimension grows. Specifically:\n",
    "\n",
    "- Evaluating $p(x_n \\mid z_n)$ requires $M$ multiplications and $M$ sampling operations.\n",
    "\n",
    "Additionally, this method can lead to **numerical underflows** due to the high number of multiplications of numbers between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3: KL Divergence Analysis**\n",
    "\n",
    "1. **Positivity of KL Divergence**:\n",
    "   Using **Jensen's inequality**, we can see that the KL divergence is always positive:\n",
    "   $$\n",
    "   D_{\\text{KL}}(p \\| q) \\geq 0\n",
    "   $$\n",
    "\n",
    "2. **Equality Case**:\n",
    "   For a very small divergence, we have $p \\approx q$.\n",
    "\n",
    "3. **Infinite Divergence**:\n",
    "   To get an infinite KL divergence, take $q$ such that the support of $p$ does not entirely cover the support of $q$.\n",
    "\n",
    "   - For **Gaussian distributions**, this is not possible because both $p$ and $q$ have support over $\\mathbb{R}$. \n",
    "   - However, we can approximate this behavior by choosing $p$ and $q$ such that:\n",
    "     - $\\mu_p$ and $\\mu_q$ (the means) are far apart, i.e., $|\\mu_p - \\mu_q|$ is large.\n",
    "     - $\\sigma_p$ and $\\sigma_q$ (the standard deviations) are small.\n",
    "\n",
    "   This results in two spikes that barely overlap, leading to a very large divergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: Rewriting $ \\log p(x_n) $\n",
    "\n",
    "The marginal likelihood $ \\log p(x_n) $ can be expressed as:\n",
    "\n",
    "$$\n",
    "\\log p(x_n) = \\log \\int p(x_n, z_n) \\, dz_n\n",
    "$$\n",
    "\n",
    "Directly computing this is challenging because of the integral over the latent variable $ z_n $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5: Pushing up the ELBO\n",
    "\n",
    "When pushing up the ELBO 2 behaviors can arise:\n",
    "\n",
    "- $ \\log p(x_n) $ can increase. In that case, our model assigns higher probability to the observed data. This directly improves the quality of the generated images.\n",
    "\n",
    "- $ KL(q(Z|x_n) \\| p(Z|x_n)) $ can decrease. In that case, we are learning a better encoder since the probability distribution learned is closer to reality.\n",
    "\n",
    "At any rate, optimizing the lower bound (ELBO) improves the model by either increasing the likelihood of the data or aligning the encoder's distribution with the true posterior, ultimately yielding better image generation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q11:\n",
    "\n",
    "Using the definition of the marginal likelihood:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\log \\int p(x, z_{1:T}) \\, dz_{1:T},\n",
    "$$\n",
    "\n",
    "we introduce the variational distribution $q_\\phi(z_{1:T} | x)$ to make the integral more tractable. By multiplying and dividing by $q_\\phi(z_{1:T} | x)$, we get:\n",
    "\n",
    "$$\n",
    "\\log p(x) = \\log \\int q_\\phi(z_{1:T} | x) \\frac{p(x, z_{1:T})}{q_\\phi(z_{1:T} | x)} \\, dz_{1:T}.\n",
    "$$\n",
    "\n",
    "Applying Jensen's inequality, we obtain:\n",
    "\n",
    "$$\n",
    "\\log p(x) \\geq \\int q_\\phi(z_{1:T} | x) \\log \\frac{p(x, z_{1:T})}{q_\\phi(z_{1:T} | x)} \\, dz_{1:T}.\n",
    "$$\n",
    "\n",
    "Simplifying the right-hand side, this becomes:\n",
    "\n",
    "$$\n",
    "\\log p(x) \\geq \\mathbb{E}_{q_\\phi(z_{1:T} | x)} \\left[ \\log p(x, z_{1:T}) - \\log q_\\phi(z_{1:T} | x) \\right].\n",
    "$$\n",
    "\n",
    "Thus, the **Evidence Lower Bound (ELBO)** for the Markovian Hierarchical Variational Autoencoder is:\n",
    "\n",
    "$$\n",
    "\\log p(x) \\geq \\mathbb{E}_{q_\\phi(z_{1:T} | x)} \\left[ \\log \\frac{p(x, z_{1:T})}{q_\\phi(z_{1:T} | x)} \\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q12:\n",
    "\n",
    "The overall architecture of the model consists in an encoder-decoder architecture where in each step of the encoding phase some noise is added to the previous image.\n",
    "The observed data point \n",
    "$x$ is treated as the initial latent variable $z_0$ in the hierarchical framework.\n",
    "\n",
    "1. During the forward diffusion process, the latent variable $z_t$ is produced at each timestep by applying a linear Gaussian transformation to $z_{t-1}$ , introducing noise as defined by the model\n",
    "\n",
    "2. During the reverse (generation) process, the VDM attempts to reconstruct the original data $x$ by iteratively denoising the latent variables $z_T, z_{t-1}...$\n",
    "\n",
    "3. At each reverse timestep, the model predicts the noise added at that step to guide the reconstruction process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
